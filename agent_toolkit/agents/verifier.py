#!/usr/bin/env python3
"""
VerifierAgent - Agent for Code Verification and Quality Assurance

This module defines the VerifierAgent, a specialized agent responsible for
verifying code generated by BuilderAgents, providing feedback, and suggesting
improvements to ensure code quality and adherence to best practices.
"""

import asyncio
import logging
import os
import re
import time
from typing import Any, Dict, List, Optional, Set, Tuple

from agent_toolkit.agents.base_agent import BaseAgent
from agent_toolkit.events import Event, EventBus, EventType
from llm_interface import LLMInterface
from project_executor import ProjectExecutor


class VerifierAgent(BaseAgent):
    """
    VerifierAgent is responsible for verifying code quality, providing feedback,
    and suggesting improvements to code generated by BuilderAgents.
    
    It works alongside BuilderAgents, verifying files as they are created and
    providing real-time feedback to improve code quality.
    """

    def __init__(self,
                agent_id: str,
                config: Dict[str, Any],
                event_bus: EventBus,
                logger: logging.Logger):
        """
        Initializes the VerifierAgent.

        Args:
            agent_id: A unique identifier for the agent instance.
            config: The configuration dictionary for the agent.
            event_bus: An instance of the EventBus for inter-agent communication.
            logger: A logger instance for the agent.
        """
        super().__init__(agent_id, config, event_bus, logger)
        
        # Set agent type for filtering
        self.config["agent_type"] = "verifier"
        
        # Initialize LLM interface for verification
        verify_config = self.config.get("verify", {})
        model_name = verify_config.get("agents", {}).get("model")
        temperature = verify_config.get("agents", {}).get("temperature", 0.2)
        # NEW: individual per-verifier goal
        self.individual_agent_goal: str = verify_config.get("agents", {}).get(
            "individual_agent_goal", ""
        )
        self.llm = LLMInterface(model_name=model_name, temperature=temperature)
        
        # Initialize project executor to read files
        project_dir = self.config.get("build", {}).get("constraints", {}).get("target_directory", "./output")
        self.executor = ProjectExecutor(project_dir)
        
        # Subscribe to relevant events
        self.event_bus.subscribe(EventType.BUILD_STEP_COMPLETED, self._handle_build_step_completed)
        
        # Track verified files to avoid duplicate work
        self.verified_files: Set[str] = set()
        
        # Store verification results in shared context
        self.verification_results_key = f"verification_results_{self.agent_id}"
        self.set_context(self.verification_results_key, {})
        
        self.logger.info(
            f"VerifierAgent {self.agent_id} initialized with model: {self.llm.model_name}"
        )
        if self.individual_agent_goal:
            self.logger.debug(
                f"VerifierAgent {self.agent_id} goal: {self.individual_agent_goal}"
            )

    async def start(self):
        """
        Starts the VerifierAgent's execution.
        Initializes verification state and prepares for verification tasks.
        """
        self.logger.info(f"VerifierAgent {self.agent_id} starting.")
        
        # Initialize verification metrics in shared context
        self.set_context(self.verification_results_key, {
            "files_verified": 0,
            "issues_found": 0,
            "suggestions_made": 0,
            "verification_summary": {}
        })
        
        # Announce verifier is ready to other agents
        await self.publish_event(EventType.VERIFY_START, {
            "agent_model": self.llm.model_name,
            "verification_strategy": self.config.get("verify", {}).get("strategy", "sequential"),
            "individual_goal": self.individual_agent_goal,
        })

    async def stop(self):
        """
        Stops the VerifierAgent's execution.
        Performs any necessary cleanup.
        """
        self.logger.info(f"VerifierAgent {self.agent_id} stopping.")
        
        # Publish verification summary
        results = self.get_context(self.verification_results_key, {})
        await self.publish_event(EventType.VERIFY_COMPLETED, {
            "files_verified": results.get("files_verified", 0),
            "issues_found": results.get("issues_found", 0),
            "suggestions_made": results.get("suggestions_made", 0)
        })

    async def _handle_build_step_completed(self, event: Event):
        """
        Handles BUILD_STEP_COMPLETED events from BuilderAgents.
        Triggers verification for newly created files.
        
        Args:
            event: The BUILD_STEP_COMPLETED event containing file information.
        """
        file_path = event.payload.get("file")
        builder_agent_id = event.agent_id
        
        if not file_path or file_path in self.verified_files:
            return
            
        self.logger.info(f"VerifierAgent {self.agent_id} received notification of completed file: {file_path}")
        
        # Add a small delay to ensure file is fully written
        await asyncio.sleep(0.5)
        
        # Verify the file
        await self._verify_file(file_path, builder_agent_id)
        
        # Mark file as verified
        self.verified_files.add(file_path)

    async def _verify_file(self, file_path: str, builder_agent_id: Optional[str]):
        """
        Verifies a single file, checking for code quality issues and suggesting improvements.
        
        Args:
            file_path: Path to the file to verify.
            builder_agent_id: ID of the BuilderAgent that created the file.
        """
        self.logger.info(f"Verifying file: {file_path}")
        verification_start = time.time()
        
        # Read the file content
        content = self.executor.read_file(file_path)
        if content is None:
            self.logger.warning(f"Could not read file for verification: {file_path}")
            return
            
        # Determine file type from extension
        file_extension = file_path.split('.')[-1] if '.' in file_path else ''
        
        # Perform verification based on file type
        if file_extension in ['py', 'js', 'ts', 'java', 'c', 'cpp', 'go', 'rs']:
            # Code file verification
            issues, suggestions = await self._verify_code_file(file_path, content, file_extension)
        elif file_extension in ['md', 'txt', 'rst']:
            # Documentation file verification
            issues, suggestions = await self._verify_documentation_file(file_path, content)
        elif file_extension in ['json', 'yaml', 'yml', 'xml']:
            # Data file verification
            issues, suggestions = await self._verify_data_file(file_path, content, file_extension)
        else:
            # Generic file verification
            issues, suggestions = await self._verify_generic_file(file_path, content)
            
        # Update verification metrics
        results = self.get_context(self.verification_results_key, {})
        results["files_verified"] = results.get("files_verified", 0) + 1
        results["issues_found"] = results.get("issues_found", 0) + len(issues)
        results["suggestions_made"] = results.get("suggestions_made", 0) + len(suggestions)
        
        # Store verification results for this file
        file_results = {
            "issues": issues,
            "suggestions": suggestions,
            "verification_time": time.time() - verification_start
        }
        results["verification_summary"][file_path] = file_results
        self.set_context(self.verification_results_key, results)
        
        # Log verification results
        self.logger.info(
            f"Verification completed for {file_path}: "
            f"{len(issues)} issues, {len(suggestions)} suggestions"
        )
        
        # Notify the builder agent if issues were found
        if issues and builder_agent_id:
            await self._notify_builder(builder_agent_id, file_path, issues, suggestions)
            
        # Publish verification event
        await self.publish_event(
            EventType.VERIFY_TEST_PASSED if not issues else EventType.VERIFY_TEST_FAILED,
            {
                "file": file_path,
                "issues_count": len(issues),
                "suggestions_count": len(suggestions),
                "verification_time_ms": (time.time() - verification_start) * 1000
            }
        )

    async def _verify_code_file(self, file_path: str, content: str, file_extension: str) -> Tuple[List[Dict], List[Dict]]:
        """
        Verifies a code file using LLM-based analysis.
        
        Args:
            file_path: Path to the file.
            content: Content of the file.
            file_extension: File extension (e.g., 'py', 'js').
            
        Returns:
            Tuple of (issues, suggestions) lists.
        """
        self.logger.debug(f"Performing code verification for {file_path}")
        
        # Create a prompt for code verification
        system_prompt = """
        You are an expert code reviewer with deep knowledge of software engineering best practices.
        Your task is to review code and identify issues and suggest improvements.
        
        Focus on these aspects:
        1. Bugs and logical errors
        2. Security vulnerabilities
        3. Performance issues
        4. Code style and readability
        5. Documentation and comments
        6. Best practices for the specific language
        """
        
        # Add individual agent goal if it exists
        if self.individual_agent_goal:
            system_prompt += f"""
            
            VERIFICATION GOAL: {self.individual_agent_goal}
            
            Keep this specific verification goal in mind as you review the code.
            """
        
        system_prompt += """
        
        Provide your analysis in JSON format with two arrays:
        - "issues": Array of objects with "severity" (critical, high, medium, low), "line" (line number), and "description"
        - "suggestions": Array of objects with "line" (line number), "description", and "improvement" (suggested code)
        """
        
        prompt = f"""
        Review the following {file_extension.upper()} code file:
        
        File: {file_path}
        
        ```{file_extension}
        {content}
        ```
        
        Analyze the code and identify any issues or potential improvements.
        Return ONLY a valid JSON object with the following structure:
        
        {{
          "issues": [
            {{"severity": "high", "line": 42, "description": "Potential SQL injection vulnerability"}}
          ],
          "suggestions": [
            {{"line": 42, "description": "Use parameterized queries", "improvement": "db.execute('SELECT * FROM users WHERE id = ?', [user_id])"}}
          ]
        }}
        
        If there are no issues or suggestions, return empty arrays.
        """
        
        # Get verification results from LLM
        response = self.llm.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2000
        )
        
        # Parse the response to extract issues and suggestions
        try:
            import json
            import re
            
            # Try to extract JSON from the response
            json_match = re.search(r'\{.*\}', response.replace('\n', ' '), re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                results = json.loads(json_str)
                issues = results.get("issues", [])
                suggestions = results.get("suggestions", [])
                
                self.logger.debug(f"Found {len(issues)} issues and {len(suggestions)} suggestions in {file_path}")
                return issues, suggestions
            else:
                self.logger.warning(f"Failed to extract JSON from LLM response for {file_path}")
                return [], []
        except Exception as e:
            self.logger.exception(f"Error parsing verification results for {file_path}: {str(e)}")
            return [], []

    async def _verify_documentation_file(self, file_path: str, content: str) -> Tuple[List[Dict], List[Dict]]:
        """
        Verifies a documentation file using LLM-based analysis.
        
        Args:
            file_path: Path to the file.
            content: Content of the file.
            
        Returns:
            Tuple of (issues, suggestions) lists.
        """
        self.logger.debug(f"Performing documentation verification for {file_path}")
        
        # Create a prompt for documentation verification
        system_prompt = """
        You are an expert technical writer with deep knowledge of documentation best practices.
        Your task is to review documentation and identify issues and suggest improvements.
        
        Focus on these aspects:
        1. Clarity and readability
        2. Completeness of information
        3. Structure and organization
        4. Grammar and spelling
        5. Technical accuracy
        """
        
        # Add individual agent goal if it exists
        if self.individual_agent_goal:
            system_prompt += f"""
            
            VERIFICATION GOAL: {self.individual_agent_goal}
            
            Keep this specific verification goal in mind as you review the documentation.
            """
        
        system_prompt += """
        
        Provide your analysis in JSON format with two arrays:
        - "issues": Array of objects with "severity" (critical, high, medium, low), "line" (line number or section), and "description"
        - "suggestions": Array of objects with "line" (line number or section), "description", and "improvement" (suggested text)
        """
        
        prompt = f"""
        Review the following documentation file:
        
        File: {file_path}
        
        ```
        {content}
        ```
        
        Analyze the documentation and identify any issues or potential improvements.
        Return ONLY a valid JSON object with the following structure:
        
        {{
          "issues": [
            {{"severity": "medium", "line": "Introduction", "description": "Missing context for new users"}}
          ],
          "suggestions": [
            {{"line": "Introduction", "description": "Add context for new users", "improvement": "This project is a microservice architecture for e-commerce platforms..."}}
          ]
        }}
        
        If there are no issues or suggestions, return empty arrays.
        """
        
        # Get verification results from LLM
        response = self.llm.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2000
        )
        
        # Parse the response to extract issues and suggestions
        try:
            import json
            import re
            
            # Try to extract JSON from the response
            json_match = re.search(r'\{.*\}', response.replace('\n', ' '), re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                results = json.loads(json_str)
                issues = results.get("issues", [])
                suggestions = results.get("suggestions", [])
                
                self.logger.debug(f"Found {len(issues)} issues and {len(suggestions)} suggestions in {file_path}")
                return issues, suggestions
            else:
                self.logger.warning(f"Failed to extract JSON from LLM response for {file_path}")
                return [], []
        except Exception as e:
            self.logger.exception(f"Error parsing verification results for {file_path}: {str(e)}")
            return [], []

    async def _verify_data_file(self, file_path: str, content: str, file_extension: str) -> Tuple[List[Dict], List[Dict]]:
        """
        Verifies a data file (JSON, YAML, etc.) using LLM-based analysis.
        
        Args:
            file_path: Path to the file.
            content: Content of the file.
            file_extension: File extension (e.g., 'json', 'yaml').
            
        Returns:
            Tuple of (issues, suggestions) lists.
        """
        self.logger.debug(f"Performing data file verification for {file_path}")
        
        # Create a prompt for data file verification
        system_prompt = """
        You are an expert in data formats and configuration files.
        Your task is to review data files and identify issues and suggest improvements.
        
        Focus on these aspects:
        1. Syntax correctness
        2. Structure and organization
        3. Completeness of required fields
        4. Consistency of naming and values
        5. Best practices for the specific format
        """
        
        # Add individual agent goal if it exists
        if self.individual_agent_goal:
            system_prompt += f"""
            
            VERIFICATION GOAL: {self.individual_agent_goal}
            
            Keep this specific verification goal in mind as you review the data file.
            """
        
        system_prompt += """
        
        Provide your analysis in JSON format with two arrays:
        - "issues": Array of objects with "severity" (critical, high, medium, low), "line" (line number or path), and "description"
        - "suggestions": Array of objects with "line" (line number or path), "description", and "improvement" (suggested data)
        """
        
        prompt = f"""
        Review the following {file_extension.upper()} data file:
        
        File: {file_path}
        
        ```{file_extension}
        {content}
        ```
        
        Analyze the data file and identify any issues or potential improvements.
        Return ONLY a valid JSON object with the following structure:
        
        {{
          "issues": [
            {{"severity": "high", "line": "config.database", "description": "Missing required connection timeout"}}
          ],
          "suggestions": [
            {{"line": "config.database", "description": "Add connection timeout", "improvement": "Add 'timeout: 30' to the database configuration"}}
          ]
        }}
        
        If there are no issues or suggestions, return empty arrays.
        """
        
        # Get verification results from LLM
        response = self.llm.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2000
        )
        
        # Parse the response to extract issues and suggestions
        try:
            import json
            import re
            
            # Try to extract JSON from the response
            json_match = re.search(r'\{.*\}', response.replace('\n', ' '), re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                results = json.loads(json_str)
                issues = results.get("issues", [])
                suggestions = results.get("suggestions", [])
                
                self.logger.debug(f"Found {len(issues)} issues and {len(suggestions)} suggestions in {file_path}")
                return issues, suggestions
            else:
                self.logger.warning(f"Failed to extract JSON from LLM response for {file_path}")
                return [], []
        except Exception as e:
            self.logger.exception(f"Error parsing verification results for {file_path}: {str(e)}")
            return [], []

    async def _verify_generic_file(self, file_path: str, content: str) -> Tuple[List[Dict], List[Dict]]:
        """
        Verifies a generic file using LLM-based analysis.
        
        Args:
            file_path: Path to the file.
            content: Content of the file.
            
        Returns:
            Tuple of (issues, suggestions) lists.
        """
        self.logger.debug(f"Performing generic verification for {file_path}")
        
        # Create a prompt for generic file verification
        system_prompt = """
        You are an expert reviewer with broad knowledge across different file types.
        Your task is to review files and identify issues and suggest improvements.
        
        Focus on these aspects:
        1. Content quality and correctness
        2. Structure and organization
        3. Consistency and completeness
        4. Best practices for the file type
        """
        
        # Add individual agent goal if it exists
        if self.individual_agent_goal:
            system_prompt += f"""
            
            VERIFICATION GOAL: {self.individual_agent_goal}
            
            Keep this specific verification goal in mind as you review the file.
            """
        
        system_prompt += """
        
        Provide your analysis in JSON format with two arrays:
        - "issues": Array of objects with "severity" (critical, high, medium, low), "line" (line number or section), and "description"
        - "suggestions": Array of objects with "line" (line number or section), "description", and "improvement" (suggested content)
        """
        
        prompt = f"""
        Review the following file:
        
        File: {file_path}
        
        ```
        {content[:2000]}  # Limit content length for very large files
        ```
        
        Analyze the file and identify any issues or potential improvements.
        Return ONLY a valid JSON object with the following structure:
        
        {{
          "issues": [
            {{"severity": "medium", "line": 10, "description": "Inconsistent formatting"}}
          ],
          "suggestions": [
            {{"line": 10, "description": "Improve formatting", "improvement": "Suggested improvement for line 10"}}
          ]
        }}
        
        If there are no issues or suggestions, return empty arrays.
        """
        
        # Get verification results from LLM
        response = self.llm.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2000
        )
        
        # Parse the response to extract issues and suggestions
        try:
            import json
            import re
            
            # Try to extract JSON from the response
            json_match = re.search(r'\{.*\}', response.replace('\n', ' '), re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                results = json.loads(json_str)
                issues = results.get("issues", [])
                suggestions = results.get("suggestions", [])
                
                self.logger.debug(f"Found {len(issues)} issues and {len(suggestions)} suggestions in {file_path}")
                return issues, suggestions
            else:
                self.logger.warning(f"Failed to extract JSON from LLM response for {file_path}")
                return [], []
        except Exception as e:
            self.logger.exception(f"Error parsing verification results for {file_path}: {str(e)}")
            return [], []

    async def _notify_builder(self, builder_agent_id: str, file_path: str, issues: List[Dict], suggestions: List[Dict]):
        """
        Notifies a builder agent about issues and suggestions for a file.
        
        Args:
            builder_agent_id: ID of the builder agent to notify.
            file_path: Path to the file with issues.
            issues: List of issues found.
            suggestions: List of suggestions for improvement.
        """
        self.logger.info(f"Notifying builder {builder_agent_id} about issues in {file_path}")
        
        # Prepare a summary of the most important issues and suggestions
        critical_issues = [i for i in issues if i.get("severity") in ["critical", "high"]]
        important_suggestions = suggestions[:3]  # Limit to top 3 suggestions
        
        # Send a direct message to the builder agent
        await self.send_message(
            to_agent=builder_agent_id,
            payload={
                "body": f"Verification results for {file_path}",
                "type": "verification_results",
                "file_path": file_path,
                "critical_issues_count": len(critical_issues),
                "total_issues_count": len(issues),
                "suggestions_count": len(suggestions),
                "critical_issues": critical_issues,
                "important_suggestions": important_suggestions
            }
        )
        
        self.logger.debug(f"Sent verification results to {builder_agent_id} for {file_path}")
